{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4219833",
   "metadata": {},
   "source": [
    "# CAB420, DCNNs, Additional Example 6: Model Variation\n",
    "Dr Simon Denman (s.denman@qut.edu.au)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Neural networks contain a lot of parameters, typically in the order of hundreds of thousands, or millions. Typically, these paraemters are randomly initialised, and then data is fed to the model is a random order. This means that each time a network is trained, you will get a slightly different network. Generally, if you're network is appropriate and you have sufficient data, results over multiple trials will be fairly similar, but they will be different. This example explores this, and looks at how we can combine multiple models to get a small increase in accuracy.\n",
    "\n",
    "It should be noted, there is a lot of research on model ensembles. This example is a very superficial look at this idea - but it is an idea that has a lot of interest and possibilities.\n",
    "\n",
    "## Too Long; Didn't Read\n",
    "\n",
    "When we train a DCNN:\n",
    "* Weights and biases are randomly initialised\n",
    "* Data is shuffled between epochs and fed to the model in a random order\n",
    "\n",
    "This means that each training run results in a slightly different model, and models will make slightly different predictions, sometimes this might be:\n",
    "* a small change in the softmax score\n",
    "* a large change in the final layer that results in a different decision\n",
    "However, assuming the model is not too huge and is trained for long enough, overall performance (i.e accuracy) will be stable across the different models.\n",
    "\n",
    "If we average model results however, we can correct some of the errors our model makes. We can also use this to help estimate model uncertainty. Although, we see a number of examples where all our models are extremely confident in their incorrect decision, so this is a not a silver bullet for estimating confidence, or correcting errors. \n",
    "\n",
    "A downside to this approach is also the increase in resources. In the ensemble we've trained here we have 10 models. Each model is simple, but we still have 10 of them. Using more models might help us better estimate uncertainty and give a small performance gain, but we will suffer diminishing returns doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8305d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738041363.235339  637200 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738041363.239563  637200 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import datetime\n",
    "import numpy\n",
    "\n",
    "import keras as keras\n",
    "from keras import layers\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25f915",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "We're using Fashion MNIST here. The setup is the same as we've used elsewhere for DCNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b023217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) / 255\n",
    "y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) / 255\n",
    "y_test = y_test.reshape(y_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50e010",
   "metadata": {},
   "source": [
    "## Network Setup\n",
    "\n",
    "We're now going to setup our network. We'll use the simple DCNN from the second DCNN example. This is fairly compact DCNN, that will train in ~2 minutes and get to about ~88-90% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe0224f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_fmnist_cnn():\n",
    "    # our input now has a different shape, 28x28x1, as we have 28x28 single channel images\n",
    "    inputs = keras.Input(shape=(28, 28, 1, ), name='img')\n",
    "    # rather than use a fully connected layer, we'll use 2D convolutional layers, 8 filters, 3x3 size kernels\n",
    "    x = layers.Conv2D(filters=8, kernel_size=(3,3), activation='relu')(inputs)\n",
    "    # 2x2 max pooling, this will downsample the image by a factor of two\n",
    "    x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "    # more convolution, 16 filters, followed by max poool\n",
    "    x = layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu')(x)\n",
    "    x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "    # final convolution, 32 filters\n",
    "    x = layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu')(x)\n",
    "    # a flatten layer. Matlab does a flatten automatically, here we need to explicitly do this. Basically we're telling\n",
    "    # keras to make the current network state into a 1D shape so we can pass it into a fully connected layer\n",
    "    x = layers.Flatten()(x)\n",
    "    # a single fully connected layer, 64 inputs\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    # and now our output, same as last time\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "    # build the model, and print the summary\n",
    "    model_cnn = keras.Model(inputs=inputs, outputs=outputs, name='fashion_mnist_cnn_model')\n",
    "    return model_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac84dd9",
   "metadata": {},
   "source": [
    "## Network Training\n",
    "\n",
    "We'll train multiple models here, simply via a for loop. I'm using the same settings I've used elsewhere, but I've set the call to ``fit`` to use ``verbose=False`` to avoid having too much text generated.\n",
    "\n",
    "All networks will be trained for the same period of time, but each network will have a different random initialisation of weights, and a different order of data fed into the network.\n",
    "\n",
    "**NOTE: Adjust number of models to be trained according to your environment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dce9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738041365.918556  637200 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1880 MB memory:  -> device: 0, name: NVIDIA A16-4Q, pci bus id: 0000:02:0b.0, compute capability: 8.6\n",
      "I0000 00:00:1738041367.645667  637233 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 2/10\n",
      "Training model 3/10\n",
      "Training model 4/10\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "histories = []\n",
    "#\n",
    "# NOTE: Adjust this parameter according to your envionment. On the QUT-hosted JupyterLab GPU Instance you will\n",
    "# need to make this smaller\n",
    "#\n",
    "num_models = 10\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(num_models):\n",
    "\n",
    "    print('Training model %d/%d' % (i + 1, num_models))\n",
    "    m = simple_fmnist_cnn()\n",
    "    m.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "               optimizer=keras.optimizers.Adam(),\n",
    "               metrics=['accuracy'], jit_compile=False)\n",
    "    h = m.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, verbose=False)\n",
    "    \n",
    "    models.append(m)\n",
    "    histories.append(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcc71f",
   "metadata": {},
   "source": [
    "### Training Results\n",
    "\n",
    "As a sanity check, we'll plot some training and validation curves for loss and accuracy. We expect to see pretty similar curves for all the models we've trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(histories):\n",
    "    fig = plt.figure(figsize=[20, 20])\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 1)\n",
    "    for i in range(num_models):\n",
    "        ax.plot(histories[i].history['loss'])\n",
    "    ax.set_title('Training Loss')\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 2)\n",
    "    for i in range(num_models):\n",
    "        ax.plot(histories[i].history['val_loss'])\n",
    "    ax.set_title('Validation Loss')\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 3)\n",
    "    for i in range(num_models):\n",
    "        ax.plot(histories[i].history['accuracy'])\n",
    "    ax.set_title('Training Accuracy')\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 4)\n",
    "    for i in range(num_models):\n",
    "        ax.plot(histories[i].history['val_accuracy'])\n",
    "    ax.set_title('Validation Accuracy')\n",
    "        \n",
    "plot_training(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae68fa",
   "metadata": {},
   "source": [
    "From the above plots, all of our models are looking fairly simmilar. Training curves look very close. There is slightly more variation for the validation curves, but accuracy is still within about 1% for all models. We can look at this a bit closer by evaluating each model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd57140",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "indexes = []\n",
    "for i in range(num_models):\n",
    "    print('Model %d' % (i + 1))\n",
    "    test_scores = models[i].evaluate(x_test, y_test, verbose=2)\n",
    "    p = models[i].predict(x_test)\n",
    "    idx = numpy.argmax(p, axis = 1)\n",
    "    preds.append(p)\n",
    "    indexes.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff408db",
   "metadata": {},
   "source": [
    "Again, we see results are, on the whole, very similar. All models are within about 1% accuracy.\n",
    "\n",
    "### An Average Model and Variation in Error Cases\n",
    "\n",
    "We can also take the average results. We can do this in a couple of ways:\n",
    "* We could take a consensus approach, i.e. look at the decision made by each model and give each model a vote\n",
    "* We could average the softmax scores, and then look the maximum value within those\n",
    "\n",
    "We'll go the second approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_prediction = numpy.mean(numpy.asarray(preds), axis = 0)\n",
    "average_idx = numpy.argmax(average_prediction, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242420af",
   "metadata": {},
   "source": [
    "We'll now look at some specific samples and see how things vary, and what the average model produces in these cases. Here, we will look at samples that the first model in our set classifies incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28045db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_check = 50\n",
    "for i in range(num_to_check):\n",
    "    if (indexes[0][i] != y_test[i]):\n",
    "        print('Index %d; True Class: %d' % (i, y_test[i]))\n",
    "        for j in range(num_models):\n",
    "            print('\\tModel %d, Predicted class %d (%f)' % (j + 1, indexes[j][i], preds[j][i][indexes[j][i]]))\n",
    "        print('\\tAverage Model, Predicted class %d (%f)' % (average_idx[i], average_prediction[i][average_idx[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2fcd3",
   "metadata": {},
   "source": [
    "What we see is that in most cases, some models in the set disagree with each other. There are situations where all or the majority of models make the same error, but even in those cases we see variation in the softmax scores. \n",
    "\n",
    "More often, for these examples we see variation in the decision of a model, and we see some models getting the answer right, and others being wrong. Looking at the results of the average model, we see that it's sometimes able to correct an error. Even in situations where this doesn't happen, the softmax score for the predicted class is a bit lower than for many of the individual models, indicating a lower amount of confidence from the model.\n",
    "\n",
    "### The Wisdom of the Crowd\n",
    "\n",
    "If we look at the accuracy of our average model (an ensemble of models), we get an interesting result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7959543",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(average_idx == y_test[:,0]) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d276b0",
   "metadata": {},
   "source": [
    "It's actually slightly more accurate than any individual model, gaining ~2% accuracy. The model is still no where near perfect, but this is a substantial jump in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = confusion_matrix(y_test, indexes[0]) \n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='g')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Label')\n",
    "ax.set_title('Model 1')\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_test, average_idx) \n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='g')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Label')\n",
    "ax.set_title('Ensemble Model');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c4f30",
   "metadata": {},
   "source": [
    "If we compare a single model to the ensemble, we see that the ensemble is not universally better across all classes. There are errors made by the ensemble that the individual model get's right - but there are more errors that are corrected.\n",
    "\n",
    "This process can be seen as being similar to the logic behind a random forest, i.e. we're taking a consensus result from a number of noisey classifiers. Of course there's a few large differences between this approach and a random forest:\n",
    "* We haven't performed bagging or other such techniques, and it's hard to imagine that our individual classifiers are independant of each other (we could try to rectify this with feature selection methods however)\n",
    "* Each of our component classifiers (i.e. each DCNN) is way more complex than an equivilent entire Random Forest; the entire ensemble is an order of magnitude more complex again.\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "Model ensembles such as what we have here are an idea that is become more popular. There are obvious downsides around the resulting complexity and training time requirements, but they also provide a means to get a small accuracy boost, and perhaps more importantly provide a better definition of uncertainty that we'd get for a softmax layer alone. \n",
    "\n",
    "The idea of estimating uncertainty (or confidence) from a DCNN is something that's becoming increasingly important as we seek to understand why a machine learning model makes the decision that it does. There are other approaches to help estimate uncertainty, such as test-time dropout, and research fields such as Bayesian Neural Networks; however these are also imperfect approaches for their own reasons.\n",
    "\n",
    "So given the above, what have we learnt today?\n",
    "\n",
    "Probably the most critical thing is that neural networks vary a fair bit across training runs. They may all end up with similar performance, but individual results (particular for hard to classify samples) can vary a lot - and the average results across a set of models can be more accurate than the individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856f429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
